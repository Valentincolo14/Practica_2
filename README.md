Pr√°ctica 2 ‚Äì Aprendizaje Autom√°tico con el Dataset Iris 

Este repositorio contiene la resoluci√≥n de 4 retos aplicando diferentes algoritmos de aprendizaje autom√°tico sobre el dataset cl√°sico Iris, ampliamente usado en clasificaci√≥n y clustering.

El objetivo de esta pr√°ctica es comparar el rendimiento de distintos modelos, entender la fundamentaci√≥n matem√°tica detr√°s de cada t√©cnica y analizar su capacidad de generalizaci√≥n.


Dataset: Iris

N√∫mero de muestras: 150

N√∫mero de caracter√≠sticas: 4 (longitud y ancho de s√©palo y p√©talo)

N√∫mero de clases: 3 (Setosa, Versicolor, Virginica)

Tarea: Clasificaci√≥n supervisada (retos 1, 2, 4) y no supervisada + red neuronal (reto 3).


Reto 1 ‚Äì Modelos Lineales, √Årboles y Ensambles

An√°lisis

En este reto se compararon tres modelos supervisados:

- Regresi√≥n Log√≠stica (lineal, probabil√≠stico).

- √Årbol de Decisi√≥n (no lineal, basado en reglas jer√°rquicas).

- Random Forest (ensamble de √°rboles, reduce sobreajuste).

Se aplic√≥ validaci√≥n cruzada, optimizaci√≥n de hiperpar√°metros con GridSearchCV y evaluaci√≥n mediante accuracy, matriz de confusi√≥n y classification report.

Fundamentaci√≥n Matem√°tica

- Regresi√≥n Log√≠stica:

<img width="329" height="126" alt="image" src="https://github.com/user-attachments/assets/081d44b6-6c8f-4517-ad9b-acdc88dd8594" />


Minimiza la funci√≥n de p√©rdida log-loss.

- √Årbol de Decisi√≥n: divide el espacio de atributos maximizando la ganancia de informaci√≥n o minimizando el √≠ndice de Gini.

- Random Forest: combina m√∫ltiples √°rboles con bagging y selecci√≥n aleatoria de atributos para mejorar la generalizaci√≥n.


Conclusiones

- La regresi√≥n log√≠stica ofrece un buen rendimiento en datasets linealmente separables como Iris.

- Los √°rboles de decisi√≥n tienden a sobreajustar, pero con poda y optimizaci√≥n logran mayor estabilidad.

- El Random Forest super√≥ en precisi√≥n a los modelos anteriores gracias a la diversidad de √°rboles.


üîπ Reto 2 ‚Äì Naive Bayes y Support Vector Machines (SVM)

An√°lisis

Se probaron dos enfoques contrastantes:

- Naive Bayes (GaussianNB): simple y r√°pido, asume independencia condicional entre variables.

- SVM: busca un hiperplano √≥ptimo de separaci√≥n, con optimizaci√≥n de C, gamma y kernel v√≠a GridSearch.

Fundamentaci√≥n Matem√°tica

Naive Bayes:


<img width="429" height="134" alt="image" src="https://github.com/user-attachments/assets/8004c08a-cf09-43f2-8ce7-acff49954ba8" />


Con ùë•ùëñ modelado por una distribuci√≥n Gaussiana.


- SVM: resuelve el problema de optimizaci√≥n:


<img width="423" height="71" alt="image" src="https://github.com/user-attachments/assets/2b8612b6-51cb-4b32-97c2-4898cb6b0caf" />

En la pr√°ctica, se extiende con kernel trick para separar datos no lineales.

Conclusiones

- Naive Bayes funciona sorprendentemente bien en Iris, pero sus supuestos limitan la capacidad predictiva.

- El SVM optimizado alcanz√≥ una precisi√≥n superior, mostrando su poder en datasets con fronteras no lineales.

- El uso de GridSearchCV fue clave para encontrar hiperpar√°metros que maximizan el rendimiento.


üîπ Reto 3 ‚Äì Clustering con K-Means y Redes Neuronales (MLP)
An√°lisis

Este reto combin√≥ un m√©todo no supervisado (K-means) y un clasificador neuronal (MLP):

- Se aplic√≥ el m√©todo del codo para determinar que ùëò=3 es el n√∫mero √≥ptimo de cl√∫steres.

- Se evalu√≥ la correspondencia entre cl√∫steres y clases reales, construyendo la matriz de confusi√≥n.

- Luego se entren√≥ un Perceptr√≥n Multicapa (MLP) con datos escalados, usando 2 capas ocultas y validaci√≥n cruzada.

Fundamentaci√≥n Matem√°tica

- K-means: minimiza la suma de distancias cuadradas dentro de los cl√∫steres (WCSS):


<img width="325" height="104" alt="image" src="https://github.com/user-attachments/assets/01d8046a-affc-46d0-8f3d-8ff64fc897c2" />


donde ùúáùëñ es el centroide del cl√∫ster.

- MLP: aplica combinaciones lineales y funciones de activaci√≥n (ùëÖùëíùêøùëà), entrenando con backpropagation:

<img width="278" height="85" alt="image" src="https://github.com/user-attachments/assets/b0d3aa5d-90f4-4363-9a42-0f2e3c371f1a" />

con actualizaci√≥n de pesos mediante el optimizador Adam.

Conclusiones

- K-means identific√≥ correctamente los 3 grupos principales del dataset, aunque con cierta confusi√≥n entre Versicolor y Virginica.

- El MLP alcanz√≥ alta precisi√≥n tras escalar los datos, confirmando la importancia del preprocesamiento en redes neuronales.

- La validaci√≥n cruzada mostr√≥ un modelo consistente y con buena capacidad de generalizaci√≥n.


üîπ Reto 4 ‚Äì Gradient Boosting y Support Vector Machine (Comparativa)

An√°lisis

Se implementaron y optimizaron dos modelos avanzados:

- Gradient Boosting Classifier (GBC): ensamble secuencial de √°rboles, optimizando n_estimators, learning_rate y max_depth.

- SVM: ajustado nuevamente, comparado con GBC.

Fundamentaci√≥n Matem√°tica

- Gradient Boosting: construye √°rboles secuenciales corrigiendo errores del anterior, minimizando una funci√≥n de p√©rdida:


<img width="355" height="65" alt="image" src="https://github.com/user-attachments/assets/09cffada-8590-4125-bb36-2f138b054ebf" />

donde Œ∑ es la learning rate y ‚Ñéùëö es el nuevo √°rbol ajustado al residuo.

- SVM: mismo planteamiento matem√°tico que en el reto 2, aqu√≠ afinado con grid search para una comparaci√≥n justa.

Conclusiones

- Ambos modelos alcanzaron una alta precisi√≥n.

- En general, Gradient Boosting se destac√≥ ligeramente al capturar relaciones m√°s complejas entre atributos.

- SVM sigue siendo competitivo, especialmente con kernels adecuados.

- La comparaci√≥n directa evidenci√≥ que los ensambles modernos (GBC, Random Forest) suelen superar a modelos individuales.


Conclusiones Generales

- El dataset Iris, aunque simple, permiti√≥ ilustrar claramente la diferencia entre modelos lineales, basados en reglas, ensambles, probabil√≠sticos, de margen m√°ximo y redes neuronales.

- Ensambladores (Random Forest, Gradient Boosting) mostraron mayor precisi√≥n y robustez.

- SVM y MLP confirmaron su potencial en problemas no lineales.

- Naive Bayes y Regresi√≥n Log√≠stica fueron r√°pidos y f√°ciles de entrenar, √∫tiles como baseline.

- K-means demostr√≥ que incluso sin etiquetas, es posible encontrar estructuras naturales en los datos.
